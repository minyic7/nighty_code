# Production Scoring Configuration

environment = "prod"

spark {
  app.name = "ELT-AAN-Scoring-Production"
  master = "yarn"
  
  sql {
    adaptive.enabled = true
    adaptive.coalescePartitions.enabled = true
    adaptive.skewJoin.enabled = true
    adaptive.localShuffleReader.enabled = true
    shuffle.partitions = 1200
  }
  
  serializer = "org.apache.spark.serializer.KryoSerializer"
  
  executor {
    memory = "16g"
    cores = 5
    instances = 50
    memoryFraction = 0.8
  }
  
  driver {
    memory = "8g"
    cores = 4
    maxResultSize = "2g"
  }
  
  dynamicAllocation {
    enabled = true
    minExecutors = 10
    maxExecutors = 100
    initialExecutors = 30
  }
}

database {
  url = "jdbc:postgresql://postgres-prod-cluster.internal:5432/scoring_prod"
  driver = "org.postgresql.Driver"
  user = ${?DB_USER}
  password = ${?DB_PASSWORD}
  
  connection_pool {
    initial_size = 20
    max_size = 100
    timeout = 60000
    validation_query = "SELECT 1"
  }
  
  read_only_replica {
    url = "jdbc:postgresql://postgres-prod-replica.internal:5432/scoring_prod"
    user = ${?DB_READONLY_USER}
    password = ${?DB_READONLY_PASSWORD}
  }
}

paths {
  input = "s3a://data-lake-prod/raw/accounts/"
  output = "s3a://data-lake-prod/processed/aan/scoring/"
  checkpoint = "s3a://data-lake-prod/checkpoints/aan/scoring/"
  
  models = "s3a://ml-models-prod/aan/scoring/"
  model_versions = "s3a://ml-models-prod/aan/versions/"
  
  archive = "s3a://data-archive-prod/aan/scoring/"
}

processing {
  batch_size = 100000
  parallelism = 32
  timeout_minutes = 480
  
  features {
    account_analysis = true
    risk_scoring = true
    notification_generation = true
    model_inference = true
    real_time_scoring = true
  }
  
  retry {
    max_attempts = 5
    backoff_seconds = 120
    exponential_backoff = true
  }
  
  checkpointing {
    enabled = true
    interval_seconds = 300
  }
}

model {
  version = "v2.1.0"
  fallback_version = "v2.0.3"
  refresh_interval_hours = 6
  
  scoring {
    threshold_high_risk = 0.85
    threshold_medium_risk = 0.65
    threshold_low_risk = 0.35
  }
}

security {
  encryption {
    enabled = true
    algorithm = "AES-256-GCM"
  }
  
  data_masking {
    enabled = true
    pii_fields = ["account_number", "customer_id", "email"]
  }
}

monitoring {
  metrics_enabled = true
  health_check_interval = 60
  alerting_enabled = true
  
  thresholds {
    error_rate_percent = 1.0
    latency_p99_ms = 10000
    throughput_min_rps = 100
  }
  
  dashboards = ["grafana", "datadog"]
}

logging {
  level = "ERROR"
  appenders = ["file", "splunk", "metrics"]
  
  file {
    path = "/opt/logs/aan-scoring-prod.log"
    max_size = "1GB"
    max_files = 50
    compression = true
  }
  
  splunk {
    host = "splunk-prod.internal"
    port = 8088
    index = "aan_scoring"
  }
}

notifications {
  enabled = true
  channels = ["slack", "email", "pagerduty"]
  
  slack {
    webhook = ${?SLACK_WEBHOOK_URL}
    channel = "#aan-alerts"
  }
  
  email {
    smtp_host = "smtp.internal.company.com"
    recipients = ["aan-team@company.com", "data-ops@company.com"]
  }
  
  pagerduty {
    service_key = ${?PAGERDUTY_SERVICE_KEY}
    escalation_timeout = 300
  }
}