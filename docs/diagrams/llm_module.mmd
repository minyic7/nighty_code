%%{init: {'theme':'dark', 'themeVariables': { 'primaryColor':'#1f2937', 'primaryTextColor':'#fff', 'primaryBorderColor':'#7C3AED', 'lineColor':'#F59E0B', 'secondaryColor':'#6366F1', 'tertiaryColor':'#10B981'}}}%%

graph TB
    %% LLM Module - Complete Provider Management System
    %% Fully implemented with all middleware and providers
    
    subgraph ClientLayer["üéØ Client Interface (core/client.py)"]
        LLMClient["LLM Client<br/>‚Ä¢ Unified interface<br/>‚Ä¢ Structured output<br/>‚Ä¢ Stream support"]
        
        Methods["Client Methods<br/>‚Ä¢ complete()<br/>‚Ä¢ stream_complete()<br/>‚Ä¢ with response_model"]
    end
    
    subgraph ManagementLayer["üéõÔ∏è Management Layer"]
        
        subgraph Manager["Manager (core/manager.py)"]
            LLMManager["LLM Manager<br/>‚Ä¢ Singleton pattern<br/>‚Ä¢ Provider registry<br/>‚Ä¢ Pool management"]
            ConfigManager["Config Manager<br/>‚Ä¢ Global config<br/>‚Ä¢ Provider configs<br/>‚Ä¢ Pool settings"]
        end
        
        subgraph ConnectionPool["Connection Pool (core/pool.py)"]
            PoolManager["Pool Manager<br/>‚Ä¢ Connection lifecycle<br/>‚Ä¢ Health checks<br/>‚Ä¢ Load balancing"]
            ConnectionSlots["Connection Slots<br/>‚Ä¢ Min/Max size<br/>‚Ä¢ Timeout control<br/>‚Ä¢ Reuse strategy"]
        end
    end
    
    subgraph MiddlewarePipeline["‚öôÔ∏è Middleware Pipeline (middleware/)"]
        
        RetryMiddleware["Retry Middleware<br/>‚Ä¢ Exponential backoff<br/>‚Ä¢ Max retries<br/>‚Ä¢ Error handling<br/>(retry.py)"]
        
        RateLimiter["Rate Limiter<br/>‚Ä¢ Token limits<br/>‚Ä¢ Request limits<br/>‚Ä¢ Time windows<br/>(rate_limiter.py)"]
        
        TokenCalculator["Token Calculator<br/>‚Ä¢ Count tokens<br/>‚Ä¢ Validate limits<br/>‚Ä¢ Cost estimation<br/>(token_calculator.py)"]
        
        MetricsCollector["Metrics Collector<br/>‚Ä¢ Latency tracking<br/>‚Ä¢ Usage stats<br/>‚Ä¢ Success rates<br/>(metrics.py)"]
        
        RequestLogger["Request Logger<br/>‚Ä¢ Request logging<br/>‚Ä¢ Response logging<br/>‚Ä¢ Error logging<br/>(logging.py)"]
    end
    
    subgraph ProviderLayer["ü§ñ Provider Layer (providers/)"]
        
        subgraph AnthropicProvider["Anthropic Provider (anthropic.py)"]
            ClaudeModels["Claude Models<br/>‚Ä¢ claude-3-opus<br/>‚Ä¢ claude-3-sonnet<br/>‚Ä¢ claude-3-haiku"]
            AnthropicClient["Anthropic Client<br/>‚Ä¢ Native SDK<br/>‚Ä¢ Async support<br/>‚Ä¢ Error handling"]
        end
        
        subgraph OpenAIProvider["OpenAI Provider (openai.py)"]
            GPTModels["GPT Models<br/>‚Ä¢ gpt-4<br/>‚Ä¢ gpt-3.5-turbo<br/>‚Ä¢ Custom models"]
            OpenAIClient["OpenAI Client<br/>‚Ä¢ Native SDK<br/>‚Ä¢ Async support<br/>‚Ä¢ Function calling"]
        end
        
        BaseProvider["Base Provider (base.py)<br/>‚Ä¢ Abstract interface<br/>‚Ä¢ Common methods<br/>‚Ä¢ Type definitions"]
    end
    
    subgraph StructuredOutput["üìä Structured Output"]
        Instructor["Instructor Integration<br/>‚Ä¢ Pydantic models<br/>‚Ä¢ JSON schema<br/>‚Ä¢ Validation<br/>‚Ä¢ Retry on failure"]
        
        ResponseModels["Response Models<br/>‚Ä¢ Type safety<br/>‚Ä¢ Auto-validation<br/>‚Ä¢ Field extraction"]
    end
    
    subgraph TypeSystem["üìù Type System (core/types.py)"]
        CoreTypes["Core Types<br/>‚Ä¢ Message<br/>‚Ä¢ MessageRole<br/>‚Ä¢ CompletionResponse<br/>‚Ä¢ StreamChunk"]
        
        ConfigTypes["Config Types<br/>‚Ä¢ LLMConfig<br/>‚Ä¢ PoolConfig<br/>‚Ä¢ RetryConfig<br/>‚Ä¢ ProviderConfig"]
        
        EnumTypes["Enum Types<br/>‚Ä¢ LLMProvider<br/>‚Ä¢ ModelType<br/>‚Ä¢ ResponseFormat"]
    end
    
    %% Main Request Flow
    LLMClient --> Methods
    Methods --> LLMManager
    
    LLMManager --> ConfigManager
    ConfigManager --> ConnectionPool
    
    ConnectionPool --> PoolManager
    PoolManager --> ConnectionSlots
    
    ConnectionSlots --> RetryMiddleware
    RetryMiddleware --> RateLimiter
    RateLimiter --> TokenCalculator
    TokenCalculator --> MetricsCollector
    MetricsCollector --> RequestLogger
    
    RequestLogger --> BaseProvider
    BaseProvider --> AnthropicProvider
    BaseProvider --> OpenAIProvider
    
    AnthropicProvider --> AnthropicClient
    OpenAIProvider --> OpenAIClient
    
    %% Structured Output Flow
    Methods -->|response_model| Instructor
    Instructor --> AnthropicClient
    Instructor --> OpenAIClient
    Instructor --> ResponseModels
    
    %% Type System Integration
    LLMClient --> CoreTypes
    ConfigManager --> ConfigTypes
    LLMManager --> EnumTypes
    
    %% Response Flow
    AnthropicClient -->|Response| ResponseModels
    OpenAIClient -->|Response| ResponseModels
    ResponseModels --> LLMClient
    
    style ClientLayer fill:#1e3a5f,stroke:#3b82f6,stroke-width:3px
    style ManagementLayer fill:#1e453f,stroke:#10b981,stroke-width:2px
    style MiddlewarePipeline fill:#312e4f,stroke:#8b5cf6,stroke-width:2px
    style ProviderLayer fill:#1e293b,stroke:#f59e0b,stroke-width:2px
    style StructuredOutput fill:#2d1b47,stroke:#ec4899,stroke-width:2px
    style TypeSystem fill:#374151,stroke:#6b7280,stroke-width:2px
    
    %% Implementation Status
    subgraph Legend["üìå Implementation Status"]
        FullyImplemented["‚úÖ All components are fully implemented and tested"]
        KeyFeatures["üîë Key Features: Multi-provider, Connection pooling, Full middleware, Structured output"]
    end
    
    style Legend fill:#1a1a2e,stroke:#666,stroke-width:1px,stroke-dasharray: 5 5