# Data Science Test Configuration

environment = "test"

spark {
  app.name = "ELT-AAN-DataScience-Test"
  master = "local[4]"
  
  sql {
    adaptive.enabled = false
    shuffle.partitions = 50
  }
  
  serializer = "org.apache.spark.serializer.KryoSerializer"
  
  executor {
    memory = "1g"
    cores = 1
  }
}

database {
  url = "jdbc:h2:mem:testdb;DB_CLOSE_DELAY=-1;MODE=PostgreSQL"
  driver = "org.h2.Driver"
  user = "test"
  password = "test"
  
  connection_pool {
    initial_size = 1
    max_size = 5
    timeout = 10000
  }
}

paths {
  input = "file:///tmp/test/data/raw/accounts/"
  output = "file:///tmp/test/data/processed/aan/"
  checkpoint = "file:///tmp/test/data/checkpoints/aan/"
  
  models = "file:///tmp/test/models/aan/"
}

processing {
  batch_size = 100
  parallelism = 2
  timeout_minutes = 5
  
  features {
    account_analysis = true
    risk_scoring = false
    notification_generation = false
  }
}

test {
  data_size = "small"
  mock_external_services = true
  fast_fail = true
}

logging {
  level = "DEBUG"
  appenders = ["console"]
}