# Nighty Code

A comprehensive AI development framework featuring modular components for LLM interaction, tool orchestration, cognitive workflows, and intelligent data extraction.

## ğŸš€ Overview

Nighty Code is a production-ready Python framework that provides four powerful modules for building sophisticated AI applications:

- **LLM Module**: Multi-provider LLM client with structured output and middleware
- **MCP Module**: Model Context Protocol implementation for tool integration
- **Copilot Module**: Cognitive workflow orchestration with memory and reasoning
- **DataMiner Module**: Intelligent data extraction from unstructured content

## ğŸ“¦ Modules

### [LLM Module](src/llm/README.md)
Unified interface for multiple LLM providers with advanced features:
- ğŸ”„ Support for Anthropic Claude and OpenAI GPT
- ğŸ“Š Structured output with Pydantic/Instructor
- ğŸŒŠ Token streaming for real-time responses
- ğŸ” Automatic retry with exponential backoff
- ğŸ“ˆ Metrics collection and monitoring
- ğŸ”Œ Connection pooling for high throughput

### [MCP Module](src/mcp/README.md)
Model Context Protocol implementation for seamless tool integration:
- ğŸ› ï¸ Dynamic tool registration and discovery
- ğŸ“ Safe filesystem operations with access control
- ğŸ” Fast code searching with grep/ripgrep
- ğŸ“¦ Resource and prompt template management
- ğŸ”„ Batch operations support
- ğŸ”’ Security features (path traversal prevention, limits)

### [Copilot Module](src/copilot/README.md)
Intelligent workflow orchestration with cognitive capabilities:
- ğŸ§  Multi-step reasoning and planning
- ğŸ’¬ Interactive conversational sessions
- ğŸ”§ Intelligent tool routing
- ğŸ“ Short and long-term memory management
- ğŸ›¡ï¸ Safety guardrails and content filtering
- ğŸ¤ Multi-agent collaboration support

### [DataMiner Module](src/dataminer/README.md)
Advanced data extraction system using LLMs:
- ğŸ“Š Multiple extraction strategies (Simple, Multi-stage, Cognitive)
- ğŸ¯ Type-safe extraction with Pydantic schemas
- ğŸ” Repository-wide code analysis
- ğŸ“ˆ Confidence scoring and gap analysis
- âš¡ Batch processing with concurrency
- ğŸ’¾ Intelligent result caching

## ğŸ› ï¸ Installation

### Prerequisites
- Python 3.8+
- pip or poetry

### Install Dependencies

```bash
# Core dependencies
pip install pydantic asyncio typing-extensions

# LLM module dependencies
pip install anthropic openai instructor tiktoken

# Additional dependencies
pip install aiofiles numpy pathlib
```

### Environment Setup

Create a `.env` file in the project root:

```bash
# API Keys
ANTHROPIC_API_KEY=your-anthropic-api-key
OPENAI_API_KEY=your-openai-api-key

# Optional configurations
DATAMINER_MODE=fast
DATAMINER_CACHE_DIR=~/.dataminer_cache
```

## ğŸš€ Quick Start

### LLM Module Example

```python
import asyncio
from src.llm import LLMManager, Message, MessageRole, LLMProvider

async def main():
    manager = LLMManager()
    client = manager.get_client(LLMProvider.ANTHROPIC)
    
    messages = [
        Message(role=MessageRole.USER, content="Explain quantum computing")
    ]
    
    response = await client.complete(messages=messages)
    print(response.content)

asyncio.run(main())
```

### MCP Module Example

```python
from src.mcp import FilesystemServer, ToolCall
from pathlib import Path

server = FilesystemServer(allowed_directories=[Path.cwd()])
await server.initialize()

result = await server.call_tool(ToolCall(
    name="read_file",
    arguments={"path": "README.md"}
))
```

### Copilot Module Example

```python
from src.copilot import CopilotClient, InteractiveSession

client = CopilotClient()
await client.initialize()

session = InteractiveSession("demo", enable_memory=True)
response = await session.chat("Help me write a Python function")
print(response.content)
```

### DataMiner Module Example

```python
from src.dataminer import DataMinerClient
from pydantic import BaseModel, Field

class PersonInfo(BaseModel):
    name: str = Field(description="Person's name")
    age: int = Field(description="Person's age")
    occupation: str = Field(description="Person's job")

client = DataMinerClient()
await client.initialize()

text = "John Smith is a 30-year-old software engineer."
result = await client.extract(content=text, schema=PersonInfo)
print(f"Name: {result.data.name}, Age: {result.data.age}")
```

## ğŸ“š Examples

Comprehensive examples for each module are available in the `examples/` directory:

- [`examples/llm_usage.py`](examples/llm_usage.py) - LLM module demonstrations
- [`examples/mcp_usage.py`](examples/mcp_usage.py) - MCP module demonstrations
- [`examples/copilot_usage.py`](examples/copilot_usage.py) - Copilot module demonstrations
- [`examples/dataminer_usage.py`](examples/dataminer_usage.py) - DataMiner module demonstrations

Run any example:
```bash
python examples/llm_usage.py
```

## ğŸ—ï¸ Project Structure

```
nighty_code/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ llm/              # LLM interaction module
â”‚   â”‚   â”œâ”€â”€ core/         # Core client and manager
â”‚   â”‚   â”œâ”€â”€ middleware/   # Retry, rate limit, metrics
â”‚   â”‚   â””â”€â”€ providers/    # Provider implementations
â”‚   â”œâ”€â”€ mcp/              # Model Context Protocol
â”‚   â”‚   â”œâ”€â”€ core/         # Base server/client
â”‚   â”‚   â”œâ”€â”€ servers/      # Server implementations
â”‚   â”‚   â””â”€â”€ utils/        # Utility functions
â”‚   â”œâ”€â”€ copilot/          # Workflow orchestration
â”‚   â”‚   â”œâ”€â”€ client/       # Interactive sessions
â”‚   â”‚   â”œâ”€â”€ core/         # Workflow engine
â”‚   â”‚   â”œâ”€â”€ guardrails/   # Safety checks
â”‚   â”‚   â”œâ”€â”€ memory/       # Memory management
â”‚   â”‚   â””â”€â”€ nodes/        # Cognitive nodes
â”‚   â””â”€â”€ dataminer/        # Data extraction
â”‚       â”œâ”€â”€ core/         # Configuration and types
â”‚       â”œâ”€â”€ models/       # Extraction schemas
â”‚       â”œâ”€â”€ strategies/   # Extraction strategies
â”‚       â””â”€â”€ utils/        # Validation and analysis
â”œâ”€â”€ examples/             # Usage examples
â”œâ”€â”€ tests/               # Test suites
â”œâ”€â”€ config/              # Configuration files
â””â”€â”€ docs/               # Documentation
```

## ğŸ”§ Configuration

### Global Configuration

Create `config/settings.yaml`:

```yaml
llm:
  default_provider: anthropic
  retry_attempts: 3
  rate_limit: 60

mcp:
  allowed_directories:
    - ./data
    - ./workspace
  max_file_size: 10485760

copilot:
  enable_memory: true
  enable_guardrails: true
  session_timeout: 300

dataminer:
  default_mode: thorough
  cache_enabled: true
  confidence_threshold: 0.7
```

### Module-Specific Configuration

Each module can be configured independently:

```python
# LLM Configuration
from src.llm import LLMConfig
config = LLMConfig(
    retry_max_attempts=3,
    rate_limit_requests_per_minute=60
)

# DataMiner Configuration
from src.dataminer import DataMinerConfig
config = DataMinerConfig(
    extraction=ExtractionConfig(
        default_mode=ProcessingMode.THOROUGH,
        min_confidence_threshold=0.7
    )
)
```

## ğŸ§ª Testing

Run the test suite:

```bash
# Run all tests
python -m pytest tests/

# Run specific module tests
python -m pytest tests/test_llm.py
python -m pytest tests/test_mcp.py
python -m pytest tests/test_copilot.py
python -m pytest tests/test_dataminer.py

# Run with coverage
python -m pytest --cov=src tests/
```

## ğŸ¤ Integration Examples

### Combining Modules

```python
# Use Copilot with DataMiner for intelligent extraction
from src.copilot import CopilotWorkflow
from src.dataminer import DataMinerClient

workflow = CopilotWorkflow()
dataminer = DataMinerClient()

# Copilot analyzes and DataMiner extracts
analysis = await workflow.execute([{
    "name": "analyze",
    "input": "Analyze this document structure"
}])

extraction = await dataminer.extract(
    content=analysis.output,
    schema=DocumentSchema
)
```

### Building an AI Assistant

```python
from src.llm import LLMManager
from src.mcp import FilesystemServer
from src.copilot import InteractiveSession

# Initialize components
llm = LLMManager()
filesystem = FilesystemServer()
session = InteractiveSession(
    enable_memory=True,
    enable_tools=True
)

# AI can now chat, use tools, and remember context
response = await session.chat("Read config.yaml and explain it")
```

## ğŸ“ˆ Performance Considerations

- **Connection Pooling**: Use connection pools for high-throughput scenarios
- **Caching**: Enable caching for repeated operations
- **Batch Processing**: Process multiple items concurrently
- **Async Operations**: Leverage async/await for non-blocking I/O
- **Resource Limits**: Set appropriate memory and rate limits

## ğŸ›¡ï¸ Security

- **API Key Management**: Never commit API keys; use environment variables
- **Access Control**: Configure allowed directories for filesystem operations
- **Input Validation**: All inputs are validated before processing
- **Guardrails**: Content filtering and safety checks are available
- **Rate Limiting**: Prevent API abuse with configurable limits

## ğŸ“ Contributing

We welcome contributions! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Anthropic for Claude API
- OpenAI for GPT API
- Instructor library for structured output
- The open-source community

## ğŸ“ Support

- **Issues**: [GitHub Issues](https://github.com/yourusername/nighty_code/issues)
- **Discussions**: [GitHub Discussions](https://github.com/yourusername/nighty_code/discussions)
- **Email**: support@nightycode.com

## ğŸš¦ Status

- âœ… LLM Module: Production Ready
- âœ… MCP Module: Production Ready
- âœ… Copilot Module: Beta
- âœ… DataMiner Module: Production Ready

---

Built with â¤ï¸ by the Nighty Code team